---

# 'hosting_infrastructure' is used to drive the correct behavior based
# on the hosting infrastructure, cloud provider, etc. Valid values are:
# - 'openstack'
# - 'aws'
# - 'azure' (Coming Soon)
# - 'gcp'
hosting_infrastructure: aws

# Cluster Environment ID to uniquely identify the environment
env_id: "<REPLACE WITH VALID ENV ID - i.e: env1>"

## AWS Provisioning Variables
# Amazon image name to use for instances.
# Note that each AWS region may utilize a different AMI for the same image.
aws_image_name: <REPLACE WITH VALID AMI>

# Define custom Master(s) API adn Console Port - Default 8443
#openshift_master_api_port: 443
#openshift_master_console_port: 443

# Define custom console and apps public DNS Name
openshift_master_default_subdomain: "apps.{{ env_id }}.{{ dns_domain }}"
openshift_master_cluster_hostname: "console.internal.{{ env_id }}.{{ dns_domain }}"
openshift_master_cluster_public_hostname: "console.{{ env_id }}.{{ dns_domain }}"

# Define infrastructure skeleton
cloud_infrastructure:
   region: <REPLACE WITH VALID REGION>
   image_name: <REPLACE WITH VALID AMI>
   masters:
     count: <REPLACE WITH NUMBER OF INSTANCES TO CREATE>
     flavor: <REPLACE WITH DESIRED FLAVOR FOR THE INSTANCE>
     zones:
     - <REPLACE WITH THE AZ(s) LIST TO PLACE THE INSTANCES IN>
     - <REPLACE WITH THE AZ(s) LIST TO PLACE THE INSTANCES IN>
     - <REPLACE WITH THE AZ(s) LIST TO PLACE THE INSTANCES IN>
     name_prefix: <REPLACE WITH DESIRED NAME PREFIX FOR THE INSTANCE>
     docker_volume_size: <REPLACE WITH THE SIZE (Gi) FOR DOCKER POOL DEVICE>
   etcdnodes:
     count: 3
     flavor: m4.xlarge
     zones:
     - eu-central-1a
     - eu-central-1b
     - eu-central-1c
     name_prefix: etcd
     docker_volume_size: 40
   appnodes:
     count: 5
     flavor: m4.xlarge
     zones:
     - eu-central-1a
     - eu-central-1b
     - eu-central-1c
     name_prefix: node
     docker_volume_size: 40
   infranodes:
     count: 2
     flavor: i3.xlarge
     zones:
     - eu-central-1a
     - eu-central-1b
     name_prefix: infra
     docker_volume_size: 40
   cnsnodes:
     count: 0
     flavor: i3.xlarge
     zones:
     - eu-central-1a
     - eu-central-1b
     - eu-central-1c
     name_prefix: cns
     docker_volume_size: 40
     gluster_volume_size: 200

# DNS configurations
# the 'dns_domain' will be used as the base domain for the deployment and has
# to be a domain that is managed by Route53 within your AWS account
# the 'dns_nameservers' is a list of DNS resolvers the cluster should use
dns_domain: "<REPLACE WITH A VALID ROUTE53 DNS DOMAIN>"
dns_nameservers:
- 8.8.8.8

# Specify the version of docker to use
#docker_version: "1.12.*"

# master(s) root (/) volume size and device
# - default values are "/dev/sda1" and "50"
#master_root_volume: "/dev/sda1"
#master_root_volume_size: 50

# etcd(s) root (/) volume size and device
# - default values are "/dev/sda1" and "50"
#etcd_root_volume: "/dev/sda1"
#etcd_root_volume_size: 50

# infra node(s) root (/) volume size and device
# - default values are "/dev/sda1" and "50"
#infra_node_root_volume: "/dev/sda1"
#infra_node_root_volume_size: 50

# app node(s) root (/) volume size and device
# - default values are "/dev/sda1" and "50"
#app_node_root_volume: "/dev/sda1"
#app_node_root_volume_size: 50

# cns node(s) root (/) volume size and device
# - default values are "/dev/sda1" and "50"
#cns_node_root_volume: "/dev/sda1"
#cns_node_root_volume_size: 50

# Docker Storage configuration variables - for all nodes
# - default values are "/dev/xvdb" and "40"
#docker_storage_block_device: "/dev/xvdb"
#docker_volume_size: "40"

# CNS (GlusterFS) Storage configuration variables
# - default values are "/dev/xvdg" and "200"
#cns_node_glusterfs_volume: "/dev/xvdg"
#cns_node_glusterfs_volume_size: "200"

# AWS access keys used by the tools to interact with ec2
# This example uses ENV variables (recommended), but the values
# can also be specified here
aws_access_key: "{{ lookup('env','AWS_ACCESS_KEY_ID') }}"
aws_secret_key: "{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}"

aws_region: <REPLACE WITH VALID AWS REGION>

# If aws_create_vpc is 'true', aws_vpc_id and aws_subnet_id variables will be ignored
aws_create_vpc: true

# Custom Subnet CIDR configuration. When selecting aws_create_vpc 'true', a default CIDR will be creted for both VPC and subnets on each AZ
# Use the following variables if you want to configure custom CIDR
#vpc_cidr: <REPLACE WITH VALID CIDR VALUE>
#subnet_az1_cidr: <REPLACE WITH VALID CIDR VALUE>
#subnet_az2_cidr: <REPLACE WITH VALID CIDR VALUE>
#subnet_az3_cdir:  <REPLACE WITH VALID CIDR VALUE>

# Custom VPC configuration. When selecting aws_create_vpc 'false', an existing VPC id must be provided and at least 1 subnet ID.
#aws_vpc_id: <REPLACE WITH VALID VPC ID>
#aws_subnet_az1_id: <REPLACE WITH VALID SUBNET ID>
#aws_subnet_az2_id: <REPLACE WITH VALID SUBNET ID>
#aws_subnet_az3_id: <REPLACE WITH VALID SUBNET ID>

# use the "-e" option to set "aws_key_name"
#aws_key_name: my-ssh-region-key

# These are the security groups created when aws_create_vpc is 'true'. Modify accordingly to your environment in case using existing VPC and SGs
aws_master_sgroups: ['ocp-ssh', 'ocp-master', 'ocp-app-node']
aws_etcd_sgroups: ['ocp-ssh', 'ocp-etcd', 'ocp-app-node']
aws_infra_node_sgroups: ['ocp-ssh', 'ocp-infra-node', 'ocp-app-node']
aws_app_node_sgroups: ['ocp-ssh', 'ocp-app-node']
aws_cns_node_sgroups: ['ocp-ssh', 'ocp-app-node', 'ocp-cns']

# Tags and names used to identify the instances
# Note: Be careful when changing these as the tools rely on some of these values
# to be set correctly.
#master_name: master
#etcd_name: etcd
#infra_node_name: infra-node
#app_node_name: app-node
#cns_node_name: cns

group_masters_tag: masters_aws
group_etcd_nodes_tag: etcd_nodes_aws
group_infra_nodes_tag: infra_nodes_aws
group_app_nodes_tag: app_nodes_aws
group_cns_nodes_tag: cns_nodes_aws

labels_masters_tag: '{"region": "default"}'
labels_etcd_nodes_tag: '{"region": "primary"}'
labels_infra_nodes_tag: '{"region": "infra"}'
labels_app_nodes_tag: '{"region": "primary"}'
labels_cns_nodes_tag: '{"region": "primary"}'


# Subscription Management Details
rhsm_register: True
rhsm_repos:
  - "rhel-7-server-rpms"
  - "rhel-7-server-ose-3.9-rpms"
  - "rhel-7-server-extras-rpms"
  - "rhel-7-fast-datapath-rpms"

# Keep this to use Red Hat Satellite:
rhsm_server_hostname: 'sat-6.example.com'
rhsm_org_id: 'CASL_ORG'
rhsm_activationkey: 'casl-latest'

# Uncomment the following to use RHSM username, password and optionally pool:
#rhsm_username: ''
#rhsm_password: ''
# leave commented out if you want to `--auto-attach` a pool
#rhsm_pool: ''

# WARNING: By default the tools will update RPMs during provisioning. If any packages are
# updated, the host(s) will reboot to ensure the correct versions are in use. This may
# NOT be desirable during an consecutive runs to just apply minor changes. If you would
# like to avoid "surprise" reboots, make sure to uncomment the following variable.
# Do NOTE that a reboot should most likely happen on initial install, so it's important
# that this variable is commented out or set to `True` for initial runs.
#update_cluster_hosts: False


# Use RHSM username, password and optionally pool:
# NOTE: use the -e option to specify on CLI instead of statically set here
rhsm_username: '<REPLACE WITH VALID RHSM USERNAME>'
rhsm_password: '<REPLACE WITH VALID RHSM PASSWORD>'
# Get RHSM required values from system variables
#rhsm_username: "{{ lookup('env','RHN_USER') }}"
#rhsm_password: "{{ lookup('env','RHN_PASSWD') }}"

# Uncomment the following `additional_list_of_packages_to_install` to list additional
# packages/RPMs to install during install
#additional_list_of_packages_to_install:
#  - rpm-1
#  - rpm-2
